from datasets import load_from_disk
import re
from datetime import datetime
import json_repair
from utils.monitor import Monitor
import time
from src.encoder import UnifiedEncoder
from openai import OpenAI
import requests
from transformers import AutoTokenizer
from tqdm import tqdm
import io
import statistics

monitor = Monitor(device_id="5")
monitor.setup()

class VLLMClient:
    def __init__(self, system_prompt="You are an excellent reading comprehension assistant. Please provide answers in JSON format.", do_sample=False, temperature=0.7):
        """ç›´æ¥ä½¿ç”¨ OpenAI å®¢æˆ·ç«¯è¿æ¥ vLLM"""
        self.client = OpenAI(
            api_key="EMPTY",
            base_url="http://localhost:8888/v1"
        )
        self.system_prompt = system_prompt
        self.do_sample = do_sample
        self.temperature = temperature
        self.device = "vllm-server"
        
        # åŠ è½½ Qwen tokenizer ç”¨äºç²¾ç¡®è®¡ç®— token æ•°é‡
        print("ğŸ”§ æ­£åœ¨åŠ è½½ Qwen tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained("/share/home/ecnuzwx/UnifiedRAG/cache/models--Qwen--Qwen3-8B")
        self.max_context_length = 40000  # Qwen3-8B çš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
        self.reserved_tokens = 1000  # ä¸ºç³»ç»Ÿæç¤ºã€é—®é¢˜å’Œå›ç­”é¢„ç•™çš„ token æ•°é‡
        
        print(f"âœ… vLLM å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ² ç”Ÿæˆå‚æ•°: do_sample={do_sample}, temperature={temperature}")
        print(f"ğŸ“ æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦: {self.max_context_length}, é¢„ç•™ token: {self.reserved_tokens}")
    
    def chat(self, text, **kwargs):
        """ç”Ÿæˆå›ç­”"""
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": text}
        ]
        
        request_params = {
            "model": "/share/home/ecnuzwx/UnifiedRAG/cache/models--Qwen--Qwen3-8B",
            "messages": messages,
            "max_tokens": 100,
            "temperature": 0.0,
            "extra_body": {
                "do_sample": False,
                "chat_template_kwargs": {"enable_thinking": False}
            }
        }
        
        response = self.client.chat.completions.create(**request_params)
        return response.choices[0].message.content.strip()
    
    def truncate_retrieved_context(self, retrieved_context, question):
        """æˆªæ–­æ£€ç´¢å†…å®¹ä»¥ç¡®ä¿ä¸è¶…è¿‡æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦"""
        # è®¡ç®—ç³»ç»Ÿæç¤ºå’Œé—®é¢˜çš„ token æ•°é‡
        system_tokens = len(self.tokenizer.encode(self.system_prompt))
        question_tokens = len(self.tokenizer.encode(question))
        
        # è®¡ç®—å¯ç”¨äºæ£€ç´¢å†…å®¹çš„æœ€å¤§ token æ•°é‡
        max_context_tokens = self.max_context_length - system_tokens - question_tokens - self.reserved_tokens
        
        # å¦‚æœæ£€ç´¢å†…å®¹çš„ token æ•°é‡è¶…è¿‡é™åˆ¶ï¼Œåˆ™è¿›è¡Œæˆªæ–­
        context_tokens = self.tokenizer.encode(retrieved_context)
        if len(context_tokens) > max_context_tokens:
            print(f"âš ï¸  æ£€ç´¢å†…å®¹è¿‡é•¿ ({len(context_tokens)} tokens)ï¼Œæˆªæ–­è‡³ {max_context_tokens} tokens")
            truncated_tokens = context_tokens[:max_context_tokens]
            retrieved_context = self.tokenizer.decode(truncated_tokens, skip_special_tokens=True)
        
        return retrieved_context

class EncoderQASystem:
    def __init__(self, 
                 dataset_path='/share/home/ecnuzwx/UnifiedRAG/LongBench-v2',
                 encoder_model_name='jina',
                 encoder_model_path='/share/home/ecnuzwx/UnifiedRAG/saved_models/2-epoch/jina-embeddings-v2-small-en/jina_epoch_1',
                 do_sample=False,
                 temperature=0.0):
        
        print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–Encoderé—®ç­”ç³»ç»Ÿ...")
        
        # åŠ è½½æ•°æ®é›†
        print(f"ğŸ“Š æ­£åœ¨åŠ è½½æ•°æ®é›†: {dataset_path}")
        self.datasets = load_from_disk(dataset_path)
        print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆï¼ŒåŒ…å«é¢†åŸŸ: {list(self.datasets.keys())}")
        
        # åˆå§‹åŒ–ç»Ÿä¸€ç¼–ç å™¨
        print(f"ğŸ”§ æ­£åœ¨åˆå§‹åŒ–ç»Ÿä¸€ç¼–ç å™¨: {encoder_model_name}")
        self.encoder_model_name = encoder_model_name
        self.encoder = UnifiedEncoder(
            model_name=encoder_model_name,
            local_model_path=encoder_model_path
        )
        
        # åˆå§‹åŒ– vLLM å®¢æˆ·ç«¯
        print(f"ğŸ”§ æ­£åœ¨åˆå§‹åŒ–LLM (vLLM API æ¨¡å¼)...")
        
        # æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€
        if not self._check_vllm_server():
            raise RuntimeError("âŒ vLLM æœåŠ¡å™¨ä¸å¯ç”¨ï¼è¯·å…ˆå¯åŠ¨ server.py æä¾›çš„æœåŠ¡")
        
        try:
            self.llm = VLLMClient(
                do_sample=do_sample, 
                temperature=temperature
            )
            print("âœ… vLLM API å®¢æˆ·ç«¯è¿æ¥æˆåŠŸ")
        except Exception as e:
            raise RuntimeError(f"âŒ vLLM è¿æ¥å¤±è´¥: {e}")
        
        print("âœ… Encoderé—®ç­”ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        
    
    def _check_vllm_server(self):
        """æ£€æŸ¥ vLLM æœåŠ¡å™¨çŠ¶æ€"""
        try:
            response = requests.get("http://localhost:8888/health", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def parse_json_response(self, response):
        """è§£æLLMè¿”å›çš„JSONå“åº”"""
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            json_str = json_match.group()
            try:
                parsed = json_repair.loads(json_str)
                return parsed
            except:
                pass
        
        try:
            parsed = json_repair.loads(response)
            return parsed
        except:
            return {"answer": "UNKNOWN"}
    
    def process_single_question(self, sample, topk_values, verbose=True):
        """å¤„ç†å•ä¸ªé—®é¢˜ï¼šç¼–ç ä¸€æ¬¡ï¼Œæ£€ç´¢ä¸€æ¬¡ï¼Œç„¶åå¯¹ä¸åŒtopkè¿›è¡Œç¡®å®šæ€§é—®ç­”
        
        Args:
            sample: é—®é¢˜æ ·æœ¬
            topk_values: topkå€¼åˆ—è¡¨ï¼Œä¾‹å¦‚ [1, 3, 5]
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        """
        question = sample['question']
        question_id = sample['_id']
        domain = sample['sub_domain'].lower()
        choices = {
            'A': sample['choice_A'],
            'B': sample['choice_B'], 
            'C': sample['choice_C'],
            'D': sample['choice_D']
        }
        context = sample['context']
        
        if verbose:
            print(f"\nğŸ“ é—®é¢˜ID: {question_id}")
            print(f"ğŸ“ é—®é¢˜: {question[:100]}...")
            print(f"ğŸ·ï¸ é¢†åŸŸ: {domain}")
        
        
        start_time = time.perf_counter()
        self.encoder.build_vector_store(context, show_progress=False)
        run_time = time.perf_counter() - start_time
        
        # è®°å½•chunksç¼–ç ç»Ÿè®¡ï¼ˆä»…æ—¶é—´ï¼‰
        chunks_encoding_stats = {
            'question_id': question_id,
            'encoding_time': run_time
        }
        
        # è·å–åŸå§‹åˆ†å—æ—¶é—´
        original_chunking_stats = {
            'chunking_time': sample.get('time', 0.0)
        }
        
        # 2. å¯¹æ¯ä¸ªtopkå€¼åˆ†åˆ«è¿›è¡Œæ£€ç´¢å’ŒLLMé—®ç­”
        results = []
        for topk in topk_values:
            # å¯¹å½“å‰topkå€¼è¿›è¡Œæ£€ç´¢
            retrieved_context = self.encoder.query(question, topk)
            
            # æˆªæ–­æ£€ç´¢å†…å®¹ä»¥ç¡®ä¿ä¸è¶…è¿‡æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
            retrieved_context = self.llm.truncate_retrieved_context(retrieved_context, question)
            
            # æ„å»ºprompt
            prompt = f"""Based on the following document content, please answer the multiple choice question.

Document Content:
{retrieved_context}

Question: {question}

Options:
A. {choices['A']}
B. {choices['B']}
C. {choices['C']}
D. {choices['D']}

Please carefully analyze the document content and select the correct answer. Respond in JSON format with the following structure:
{{
    "answer": "A/B/C/D"
}}"""

            # è°ƒç”¨LLMç”Ÿæˆç­”æ¡ˆï¼ˆç¡®å®šæ€§è¿è¡Œï¼‰
            response = self.llm.chat(prompt)
            
            # è§£æJSONå“åº”
            parsed_response = self.parse_json_response(response)
            
            if isinstance(parsed_response, dict):
                model_answer = parsed_response.get('answer', 'UNKNOWN')
            else:
                model_answer = 'UNKNOWN'
            
            # ä¿å­˜ç»“æœ
            result = {
                'question_id': question_id,
                'question': question,
                'domain': domain,
                'topk': topk,
                'retrieved_context': retrieved_context,
                'original_chunking_stats': original_chunking_stats,
                'chunks_encoding_stats': chunks_encoding_stats,
                'raw_response': response,
                'parsed_response': parsed_response,
                'model_answer': model_answer,
                'correct_answer': sample['answer'],
                'choices': choices,
                'is_correct': model_answer.upper() == sample['answer'].upper()
            }
            
            results.append(result)
            
            # æ˜¾ç¤ºæ¯ä¸ªå›ç­”çš„å¯¹é”™
            if verbose:
                print(f"      TopK={topk}: {'âœ… æ­£ç¡®' if result['is_correct'] else 'âŒ é”™è¯¯'} (ç­”æ¡ˆ: {model_answer} / æ­£ç¡®: {sample['answer']})")
        
        return results
    
    def evaluate_single_run(self, topk_values=[1, 3, 5], verbose=False, save_results=True):
        """è¯„ä¼°æ‰€æœ‰æ ·æœ¬ - ç¡®å®šæ€§å•æ¬¡è¿è¡Œ"""
        
        print("ğŸ¯ è¿è¡Œç¡®å®šæ€§è¯„ä¼°æ¨¡å¼")
        print(f"ğŸ¯ TopKå€¼åˆ—è¡¨: {topk_values}")
        
        # è®¡ç®—æ€»æ ·æœ¬æ•°
        total_samples_all = sum(len(self.datasets[domain]) for domain in self.datasets)
        print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {total_samples_all}")
        print(f"ğŸ“Š æ¯ä¸ªæ ·æœ¬å°†è¿›è¡Œ: {len(topk_values)} ä¸ªtopk æ¬¡é—®ç­”")
        
        # å­˜å‚¨æ‰€æœ‰ç»“æœ
        all_results = []
        processed_count = 0
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_questions = 0
        total_time = 0.0
        domain_stats = {}
        
        # æŒ‰é¢†åŸŸå¤„ç†
        for domain_idx, domain_name in enumerate(self.datasets, 1):
            domain_dataset = self.datasets[domain_name]
            total_samples = len(domain_dataset)
            
            print(f"\n{'='*80}")
            print(f"ğŸ“‹ é¢†åŸŸ [{domain_idx}/{len(self.datasets)}]: {domain_name} ({total_samples}ä¸ªæ ·æœ¬)")
            print('='*80)
            
            # åˆå§‹åŒ–é¢†åŸŸç»Ÿè®¡
            domain_stats[domain_name] = {
                'total_questions': total_samples,
                'total_time': 0.0,
                'topk_stats': {topk: {'correct': 0, 'total': 0} for topk in topk_values}
            }
            
            # å¤„ç†è¯¥é¢†åŸŸçš„æ‰€æœ‰æ ·æœ¬
            for i in tqdm(range(total_samples), desc=f"Processing {domain_name}", unit="sample"):
                sample = domain_dataset[i]
                processed_count += 1
                
                # å¯¹è¯¥æ ·æœ¬è¿›è¡Œä¸€æ¬¡ç¼–ç ã€ä¸€æ¬¡æ£€ç´¢ï¼Œç„¶åç¡®å®šæ€§é—®ç­”
                sample_results = self.process_single_question(sample, topk_values, verbose=False)
                
                # å°†ç»“æœç›´æ¥æ·»åŠ åˆ°æ€»ç»“æœä¸­
                all_results.extend(sample_results)
                
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                total_questions += 1
                
                # è®¡ç®—è¯¥æ ·æœ¬çš„æ—¶é—´ï¼ˆä»chunks_encoding_statsä¸­è·å–ï¼‰
                if sample_results:
                    sample_time = sample_results[0].get('chunks_encoding_stats', {}).get('encoding_time', 0.0)
                    # åŠ ä¸ŠåŸå§‹åˆ†å—æ—¶é—´ï¼ˆå¦‚æœæœ‰ï¼‰
                    original_time = float(sample.get('time', 0.0))
                    total_sample_time = sample_time + original_time
                    
                    total_time += total_sample_time
                    domain_stats[domain_name]['total_time'] += total_sample_time
                    
                    # å°†æ€»æ—¶é—´æ³¨å…¥ç»“æœ
                    for res in sample_results:
                        res['total_sample_time'] = total_sample_time
                
                # æ›´æ–°æ­£ç¡®ç‡ç»Ÿè®¡
                for result in sample_results:
                    topk = result['topk']
                    is_correct = result['is_correct']
                    domain_stats[domain_name]['topk_stats'][topk]['total'] += 1
                    if is_correct:
                        domain_stats[domain_name]['topk_stats'][topk]['correct'] += 1
            
            # æ‰“å°å½“å‰é¢†åŸŸçš„ç»Ÿè®¡ç»“æœ
            if verbose or True:  # å¼ºåˆ¶æ‰“å°
                print(f"\nğŸ“Š é¢†åŸŸ {domain_name} è¯„ä¼°ç»“æœ:")
                stats = domain_stats[domain_name]
                print(f"    é—®é¢˜æ•°: {stats['total_questions']}")
                print(f"    æ€»è€—æ—¶: {stats['total_time']:.2f}s")
                avg_time = stats['total_time']/stats['total_questions'] if stats['total_questions'] > 0 else 0
                print(f"    å¹³å‡è€—æ—¶: {avg_time:.2f}s")
                for topk in topk_values:
                    topk_stat = stats['topk_stats'][topk]
                    accuracy = topk_stat['correct'] / topk_stat['total'] if topk_stat['total'] > 0 else 0
                    print(f"    TopK-{topk}: {accuracy:.2%} ({topk_stat['correct']}/{topk_stat['total']})")
                print("-" * 60)
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        overall_stats = {
            'total_questions': total_questions,
            'total_time': total_time,
            'avg_time_per_question': total_time / total_questions if total_questions > 0 else 0,
            'topk_accuracy': {}
        }
        
        # è®¡ç®—æ¯ä¸ªTopKçš„æ€»ä½“å‡†ç¡®ç‡
        for topk in topk_values:
            total_correct = sum(domain_stats[domain]['topk_stats'][topk]['correct'] for domain in domain_stats)
            total_count = sum(domain_stats[domain]['topk_stats'][topk]['total'] for domain in domain_stats)
            accuracy = total_correct / total_count if total_count > 0 else 0
            overall_stats['topk_accuracy'][topk] = {
                'correct': total_correct,
                'total': total_count,
                'accuracy': accuracy
            }
        
        # æ‰“å°ç»Ÿè®¡ç»“æœ
        if verbose:
            print(f"\n{'='*60}")
            print(f"ğŸ“Š Qwen3ç¼–ç å™¨è¯„ä¼°ç»“æœç»Ÿè®¡")
            print(f"{'='*60}")
            print(f"æ€»é—®é¢˜æ•°: {total_questions}")
            print(f"æ€»è€—æ—¶: {total_time:.2f}s")
            print(f"å¹³å‡æ¯é¢˜è€—æ—¶: {overall_stats['avg_time_per_question']:.2f}s")
            
            print(f"\nğŸ¯ æ€»ä½“å‡†ç¡®ç‡:")
            for topk in topk_values:
                stats = overall_stats['topk_accuracy'][topk]
                print(f"  TopK-{topk}: {stats['accuracy']:.2%}Â±{0:.2%} ({stats['correct']}/{stats['total']})")
            
            print(f"\nğŸ“‹ å„é¢†åŸŸè¯¦ç»†ç»Ÿè®¡:")
            for domain, stats in domain_stats.items():
                print(f"  {domain}:")
                print(f"    é—®é¢˜æ•°: {stats['total_questions']}")
                print(f"    æ€»è€—æ—¶: {stats['total_time']:.2f}s")
                print(f"    å¹³å‡è€—æ—¶: {stats['total_time']/stats['total_questions']:.2f}s")
                for topk in topk_values:
                    topk_stat = stats['topk_stats'][topk]
                    accuracy = topk_stat['correct'] / topk_stat['total'] if topk_stat['total'] > 0 else 0
                    print(f"    TopK-{topk}: {accuracy:.2%} ({topk_stat['correct']}/{topk_stat['total']})")
        
        print(f"\n{'='*80}")
        print(f"âœ… æ‰€æœ‰æ ·æœ¬å¤„ç†å®Œæˆ: {processed_count}/{total_samples_all}")
        print(f"âœ… æ€»é—®ç­”æ¬¡æ•°: {len(all_results)}")
        print('='*80)
        
        # ä¿å­˜ç»“æœ
        if save_results:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            results_file = f"{self.encoder_model_name}_qa_results_{timestamp}.md"
            lines = []
            lines.append(f"# {self.encoder_model_name} ç¼–ç å™¨é—®ç­”ç»“æœ ({timestamp})")
            lines.append("")
            lines.append("## æ€»ä½“å‡†ç¡®ç‡")
            lines.append("")
            lines.append("| TopK | Correct | Total | Accuracy |")
            lines.append("| --- | ---: | ---: | ---: |")
            for topk in topk_values:
                stats = overall_stats['topk_accuracy'][topk]
                lines.append(f"| {topk} | {stats['correct']} | {stats['total']} | {stats['accuracy']*100:.2f}% |")
            lines.append("")
            lines.append("## å„é¢†åŸŸè¯¦ç»†ç»Ÿè®¡")
            headers = ["é¢†åŸŸ", "é—®é¢˜æ•°", "æ€»è€—æ—¶(s)", "å¹³å‡è€—æ—¶(s)"]
            for topk in topk_values:
                headers.append(f"TopK-{topk} æ­£ç¡®/æ€»æ•°")
                headers.append(f"TopK-{topk} å‡†ç¡®ç‡")
            lines.append("| " + " | ".join(headers) + " |")
            lines.append("| " + " | ".join(["---"] * len(headers)) + " |")
            for domain_name, stats in domain_stats.items():
                avg_time = stats['total_time'] / stats['total_questions'] if stats['total_questions'] > 0 else 0
                row = [
                    domain_name,
                    str(stats['total_questions']),
                    f"{stats['total_time']:.2f}",
                    f"{avg_time:.2f}",
                ]
                for topk in topk_values:
                    correct = stats['topk_stats'][topk]['correct']
                    total = stats['topk_stats'][topk]['total']
                    acc = (correct / total) if total > 0 else 0
                    row.append(f"{correct}/{total}")
                    row.append(f"{acc*100:.2f}%")
                lines.append("| " + " | ".join(row) + " |")
            with open(results_file, 'w', encoding='utf-8') as f:
                f.write("\n".join(lines))
            print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜ä¸ºMarkdownè¡¨æ ¼: {results_file}")
        
        return all_results, domain_stats



def check_vllm_server():
    """æ£€æŸ¥ vLLM æœåŠ¡å™¨æ˜¯å¦å¯ç”¨"""
    print("ğŸ” æ£€æŸ¥ vLLM æœåŠ¡å™¨çŠ¶æ€...")
    try:
        response = requests.get("http://localhost:8888/health", timeout=5)
        if response.status_code == 200:
            print("âœ… vLLM æœåŠ¡å™¨è¿è¡Œæ­£å¸¸")
            return True
    except:
        pass
    
    print("âŒ vLLM æœåŠ¡å™¨ä¸å¯ç”¨ï¼")
    print("ğŸ“‹ è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤å¯åŠ¨æœåŠ¡å™¨ï¼š")
    print("   1. è¿è¡Œ: sbatch server.sh")
    print("   2. ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨å®Œæˆ")
    print("   3. æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€: curl http://localhost:8888/health")
    raise RuntimeError("vLLM æœåŠ¡å™¨ä¸å¯ç”¨ï¼Œè¯·å…ˆå¯åŠ¨ server.py")

def main():
    print("ğŸ¯ Encoderé—®ç­”ç³»ç»Ÿ - ç¡®å®šæ€§è¯„ä¼° (vLLM API æ¨¡å¼)")
    print("=" * 60)
    check_vllm_server()
    topk_values = [5, 10]
    dataset_path = '/share/home/ecnuzwx/UnifiedRAG/LongBench-v2'
    scenarios = [
        {
            'name': 'jina',
            'path': '/share/home/ecnuzwx/UnifiedRAG/saved_models/2-epoch/jina-embeddings-v2-small-en/jina_epoch_1'
        },
        {
            'name': 'nomic-embed-text-v1.5',
            'path': '/share/home/ecnuzwx/UnifiedRAG/saved_models/2-epoch/nomic-embed-text-v1.5/xlmroberta_epoch_1'
        },
        {
            'name': 'bge-m3',
            'path': '/share/home/ecnuzwx/UnifiedRAG/saved_models/2-epoch/bge-m3/xlmroberta_epoch_1'
        },
    ]
    repeats = 1
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    summary_file = f"freechunker_qa_summary_{timestamp}.md"
    markdown_buffer = io.StringIO()
    
    for sc in scenarios:
        print("\nğŸš€ åˆå§‹åŒ–ç³»ç»Ÿ...")
        print(f"\nğŸ“‹ è¯„ä¼°é…ç½®:")
        print(f"   ç¼–ç å™¨: {sc['name']}")
        print(f"   TopKå€¼: {topk_values}")
        domain_stats_runs = []
        for _ in range(repeats):
            qa_system = EncoderQASystem(
                dataset_path=dataset_path,
                encoder_model_name=sc['name'],
                encoder_model_path=sc['path'],
                do_sample=False,
                temperature=0.0
            )
            results, domain_stats = qa_system.evaluate_single_run(topk_values=topk_values, verbose=False, save_results=False)
            domain_stats_runs.append(domain_stats)
        domains = list(domain_stats_runs[0].keys())
        total_questions_runs = [sum(d['total_questions'] for d in r.values()) for r in domain_stats_runs]
        total_time_runs = [sum(d['total_time'] for d in r.values()) for r in domain_stats_runs]
        topk_acc_runs = {k: [] for k in topk_values}
        for r in domain_stats_runs:
            for k in topk_values:
                corr = sum(d['topk_stats'][k]['correct'] for d in r.values())
                tot = sum(d['topk_stats'][k]['total'] for d in r.values())
                acc = (corr / tot) if tot > 0 else 0.0
                topk_acc_runs[k].append(acc)
        mq = statistics.mean(total_questions_runs)
        mt = statistics.mean(total_time_runs)
        st = statistics.stdev(total_time_runs) if len(total_time_runs) > 1 else 0.0
        avg_list = [t / q if q > 0 else 0.0 for t, q in zip(total_time_runs, total_questions_runs)]
        ma = statistics.mean(avg_list)
        sa = statistics.stdev(avg_list) if len(avg_list) > 1 else 0.0
        
        # Capture stats to string for markdown
        stats_buffer = io.StringIO()
        def log_stats(msg):
            print(msg) # To console
            print(msg, file=stats_buffer) # To buffer
        
        log_stats(f"\n{'='*60}")
        log_stats(f"ğŸ“Š FreeChunkerè¯„ä¼°ç»“æœç»Ÿè®¡ï¼ˆ{repeats}æ¬¡å¹³å‡Â±æ ‡å‡†å·®ï¼‰")
        log_stats(f"{'='*60}")
        log_stats(f"ç¼–ç å™¨: {sc['name']}")
        log_stats(f"æ€»é—®é¢˜æ•°: {int(mq)}")
        log_stats(f"æ€»è€—æ—¶: {mt:.2f}sÂ±{st:.2f}s")
        log_stats(f"å¹³å‡æ¯é¢˜è€—æ—¶: {ma:.2f}sÂ±{sa:.2f}s")
        log_stats(f"\nğŸ¯ æ€»ä½“å‡†ç¡®ç‡:")
        for k in topk_values:
            macc = statistics.mean(topk_acc_runs[k])
            sacc = statistics.stdev(topk_acc_runs[k]) if len(topk_acc_runs[k]) > 1 else 0.0
            log_stats(f"  TopK-{k}: {macc*100:.2f}%Â±{sacc*100:.2f}%")
        log_stats(f"\nğŸ“‹ å„é¢†åŸŸè¯¦ç»†ç»Ÿè®¡:")
        for domain in domains:
            tts = [r[domain]['total_time'] for r in domain_stats_runs]
            tqs = [r[domain]['total_questions'] for r in domain_stats_runs]
            ats = [t / q if q > 0 else 0.0 for t, q in zip(tts, tqs)]
            mt_domain = statistics.mean(tts)
            st_domain = statistics.stdev(tts) if len(tts) > 1 else 0.0
            ma_domain = statistics.mean(ats)
            sa_domain = statistics.stdev(ats) if len(ats) > 1 else 0.0
            log_stats(f"  {domain}:")
            log_stats(f"    é—®é¢˜æ•°: {int(statistics.mean(tqs))}")
            log_stats(f"    æ€»è€—æ—¶: {mt_domain:.2f}sÂ±{st_domain:.2f}s")
            log_stats(f"    å¹³å‡è€—æ—¶: {ma_domain:.2f}sÂ±{sa_domain:.2f}s")
            for k in topk_values:
                accs = []
                for r in domain_stats_runs:
                    c = r[domain]['topk_stats'][k]['correct']
                    t = r[domain]['topk_stats'][k]['total']
                    accs.append((c / t) if t > 0 else 0.0)
                macc_d = statistics.mean(accs)
                sacc_d = statistics.stdev(accs) if len(accs) > 1 else 0.0
                log_stats(f"    TopK-{k}: {macc_d*100:.2f}%Â±{sacc_d*100:.2f}%")
        
        markdown_buffer.write(stats_buffer.getvalue())
    
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write(markdown_buffer.getvalue())
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜ä¸ºMarkdown: {summary_file}")

if __name__ == "__main__":
    main()
