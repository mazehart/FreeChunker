
import torch
import numpy as np
from datasets import load_from_disk
from sklearn.metrics.pairwise import cosine_similarity
import re
from datetime import datetime
import json_repair
from utils.monitor import Monitor
import requests
import time
from tqdm import tqdm
from utils.vllm_client import VLLMClient
from utils.eval_framework import truncate_chunks_by_topk, build_mcq_prompt

class EmbeddingModel:
    def __init__(self, model_path):
        from sentence_transformers import SentenceTransformer
        print(f"ğŸ”§ æ­£åœ¨åŠ è½½embeddingæ¨¡å‹: {model_path}")
        
        # ä½¿ç”¨sentence-transformersï¼Œè‡ªåŠ¨å¤„ç†æ‰¹æ¬¡å’Œæ˜¾å­˜ç®¡ç†
        self.model = SentenceTransformer(model_path, trust_remote_code=True)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
        self.model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œç¦ç”¨dropoutç­‰è®­ç»ƒç›¸å…³å±‚
        
        print(f"âœ… Embeddingæ¨¡å‹åŠ è½½å®Œæˆï¼Œè®¾å¤‡: {self.device}")
        try:
            print(f"ğŸ“ æ¨¡å‹æœ€å¤§åºåˆ—é•¿åº¦: {self.model.max_seq_length}")
            print(f"ğŸ¯ æ¨¡å‹ç»´åº¦: {self.model.get_sentence_embedding_dimension()}")
        except Exception:
            pass
    
    def encode(self, texts, batch_size=6):
        """ç¼–ç æ–‡æœ¬"""
        embeddings = self.model.encode(
            texts,
            batch_size=batch_size,
            show_progress_bar=False,
            convert_to_numpy=True,
            normalize_embeddings=True  # å½’ä¸€åŒ–embedding
        )
        return embeddings

class BaseQASystem:
    def __init__(self, dataset_path, device_id="0", system_name="Base"):
        print(f"ğŸš€ æ­£åœ¨åˆå§‹åŒ–{system_name}é—®ç­”ç³»ç»Ÿ...")
        
        # åŠ è½½æ•°æ®é›†
        print(f"ğŸ“Š æ­£åœ¨åŠ è½½æ•°æ®é›†: {dataset_path}")
        self.datasets = load_from_disk(dataset_path)
        print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆï¼ŒåŒ…å«é¢†åŸŸ: {list(self.datasets.keys())}")
        
        # åˆå§‹åŒ–ç›‘æ§å™¨
        self.monitor = Monitor(device_id=device_id)
        self.monitor.setup()
        
        # åˆå§‹åŒ– vLLM å®¢æˆ·ç«¯
        print(f"ğŸ”§ æ­£åœ¨åˆå§‹åŒ–LLM (vLLM API æ¨¡å¼)...")
        
        # æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€
        if not self._check_vllm_server():
            raise RuntimeError("âŒ vLLM æœåŠ¡å™¨ä¸å¯ç”¨ï¼è¯·å…ˆå¯åŠ¨ server.py æä¾›çš„æœåŠ¡")
        
        try:
            self.llm = VLLMClient()
            print("âœ… vLLM API å®¢æˆ·ç«¯è¿æ¥æˆåŠŸ")
        except Exception as e:
            raise RuntimeError(f"âŒ vLLM è¿æ¥å¤±è´¥: {e}")
            
        self.system_name = system_name
        print(f"âœ… {system_name}é—®ç­”ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

    def _check_vllm_server(self):
        """æ£€æŸ¥ vLLM æœåŠ¡å™¨çŠ¶æ€"""
        try:
            response = requests.get("http://localhost:8888/health", timeout=5)
            return response.status_code == 200
        except:
            return False

    def parse_json_response(self, response):
        """è§£æLLMè¿”å›çš„JSONå“åº”"""
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            json_str = json_match.group()
            try:
                parsed = json_repair.loads(json_str)
                return parsed
            except:
                pass
        
        try:
            parsed = json_repair.loads(response)
            return parsed
        except:
            return {"answer": "UNKNOWN"}

    def process_single_question(self, sample, topk_values, verbose=True):
        raise NotImplementedError("Subclasses must implement process_single_question")

    def evaluate_single_run(self, topk_values=[1, 3, 5], verbose=False, save_results=True, results_prefix="qa"):
        """å•æ¬¡è¯„ä¼°è¿è¡Œ - æµ‹è¯•æ‰€æœ‰æ ·æœ¬ï¼ˆç¡®å®šæ€§è¾“å‡ºï¼‰"""
        print(f"ğŸ¯ {self.system_name} ç¡®å®šæ€§è¯„ä¼°æ¨¡å¼")
        print(f"ğŸ¯ TopKå€¼åˆ—è¡¨: {topk_values}")
        
        # å­˜å‚¨æ‰€æœ‰ç»“æœ
        all_results = []
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_questions = 0
        total_time = 0.0
        domain_stats = {}
        
        # æŒ‰é¢†åŸŸå¤„ç†
        for domain_name in self.datasets:
            if verbose:
                print(f"\nğŸ“‹ å¤„ç†é¢†åŸŸ: {domain_name}")
            
            domain_dataset = self.datasets[domain_name]
            total_samples = len(domain_dataset)
            total_questions += total_samples
            
            print(f"    å¤„ç†{domain_name}: {total_samples}ä¸ªæ ·æœ¬")
            
            # åˆå§‹åŒ–é¢†åŸŸç»Ÿè®¡
            domain_stats[domain_name] = {
                'total_questions': total_samples, # Unify key name
                'total_samples': total_samples,   # Keep both for compatibility if needed
                'total_time': 0.0,
                'topk_stats': {topk: {'correct': 0, 'total': 0} for topk in topk_values}
            }
            
            # å¤„ç†è¯¥é¢†åŸŸçš„æ‰€æœ‰æ ·æœ¬
            for i in tqdm(range(total_samples), desc=f"Processing {domain_name}", unit="sample"):
                sample = domain_dataset[i]
                
                # å¯¹è¯¥æ ·æœ¬è¿›è¡Œå¤„ç†
                sample_results = self.process_single_question(sample, topk_values, verbose=False)
                
                # å°†ç»“æœç›´æ¥æ·»åŠ åˆ°æ€»ç»“æœä¸­
                all_results.extend(sample_results)
                
                # è®¡ç®—è¯¥æ ·æœ¬çš„æ—¶é—´ï¼ˆä»chunks_encoding_statsä¸­è·å–ï¼‰
                if sample_results:
                    # Try to get time from chunks_encoding_stats, fallback to other means if needed
                    # StandardQASystem puts 'time' in chunks_encoding_stats
                    # FreeChunker puts 'encoding_time' in chunks_encoding_stats and also has 'original_chunking_stats'
                    
                    stats = sample_results[0].get('chunks_encoding_stats', {})
                    sample_time = stats.get('time', stats.get('encoding_time', 0.0))
                    
                    # For FreeChunker, add original time if not already included
                    if 'encoding_time' in stats: 
                         original_time = float(sample.get('time', 0.0))
                         sample_time += original_time
                         
                    total_time += sample_time
                    domain_stats[domain_name]['total_time'] += sample_time
                
                # æ›´æ–°æ­£ç¡®ç‡ç»Ÿè®¡
                for result in sample_results:
                    topk = result['topk']
                    is_correct = result['is_correct']
                    domain_stats[domain_name]['topk_stats'][topk]['total'] += 1
                    if is_correct:
                        domain_stats[domain_name]['topk_stats'][topk]['correct'] += 1
            
            print(f"  âœ… {domain_name} å®Œæˆ: {total_samples}/{total_samples} (100%)")
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        overall_stats = {
            'total_questions': total_questions,
            'total_time': total_time,
            'avg_time_per_question': total_time / total_questions if total_questions > 0 else 0,
            'topk_accuracy': {}
        }
        
        # è®¡ç®—æ¯ä¸ªTopKçš„æ€»ä½“å‡†ç¡®ç‡
        for topk in topk_values:
            total_correct = sum(domain_stats[domain]['topk_stats'][topk]['correct'] for domain in domain_stats)
            total_count = sum(domain_stats[domain]['topk_stats'][topk]['total'] for domain in domain_stats)
            accuracy = total_correct / total_count if total_count > 0 else 0
            overall_stats['topk_accuracy'][topk] = {
                'correct': total_correct,
                'total': total_count,
                'accuracy': accuracy
            }
            
        # æ‰“å°ç»Ÿè®¡ç»“æœ
        if verbose or True: # Always print summary at end of run
            print(f"\n{'='*60}")
            print(f"ğŸ“Š {self.system_name} è¯„ä¼°ç»“æœç»Ÿè®¡")
            print(f"{'='*60}")
            print(f"æ€»é—®é¢˜æ•°: {total_questions}")
            print(f"æ€»è€—æ—¶: {total_time:.2f}s")
            print(f"å¹³å‡æ¯é¢˜è€—æ—¶: {overall_stats['avg_time_per_question']:.2f}s")
            
            print(f"\nğŸ¯ æ€»ä½“å‡†ç¡®ç‡:")
            for topk in topk_values:
                stats = overall_stats['topk_accuracy'][topk]
                print(f"  TopK-{topk}: {stats['accuracy']:.2%} ({stats['correct']}/{stats['total']})")
            
            print(f"\nğŸ“‹ å„é¢†åŸŸè¯¦ç»†ç»Ÿè®¡:")
            for domain, stats in domain_stats.items():
                print(f"  {domain}:")
                print(f"    é—®é¢˜æ•°: {stats['total_questions']}")
                print(f"    æ€»è€—æ—¶: {stats['total_time']:.2f}s")
                avg_time = stats['total_time']/stats['total_questions'] if stats['total_questions'] > 0 else 0
                print(f"    å¹³å‡è€—æ—¶: {avg_time:.2f}s")
                for topk in topk_values:
                    topk_stat = stats['topk_stats'][topk]
                    accuracy = topk_stat['correct'] / topk_stat['total'] if topk_stat['total'] > 0 else 0
                    print(f"    TopK-{topk}: {accuracy:.2%} ({topk_stat['correct']}/{topk_stat['total']})")
        
        # ä¿å­˜ç»“æœ
        if save_results:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            results_file = f"{results_prefix}_results_{timestamp}.md"
            self._save_markdown_results(results_file, topk_values, overall_stats, domain_stats, timestamp)
            print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜ä¸ºMarkdownè¡¨æ ¼: {results_file}")
        
        return all_results, domain_stats

    def _save_markdown_results(self, filename, topk_values, overall_stats, domain_stats, timestamp):
        lines = []
        lines.append(f"# {self.system_name} é—®ç­”ç»“æœ ({timestamp})")
        lines.append("")
        lines.append("## æ€»ä½“å‡†ç¡®ç‡")
        lines.append("")
        lines.append("| TopK | Correct | Total | Accuracy |")
        lines.append("| --- | ---: | ---: | ---: |")
        for topk in topk_values:
            stats = overall_stats['topk_accuracy'][topk]
            lines.append(f"| {topk} | {stats['correct']} | {stats['total']} | {stats['accuracy']*100:.2f}% |")
        lines.append("")
        lines.append("## å„é¢†åŸŸè¯¦ç»†ç»Ÿè®¡")
        headers = ["é¢†åŸŸ", "é—®é¢˜æ•°", "æ€»è€—æ—¶(s)", "å¹³å‡è€—æ—¶(s)"]
        for topk in topk_values:
            headers.append(f"TopK-{topk} æ­£ç¡®/æ€»æ•°")
            headers.append(f"TopK-{topk} å‡†ç¡®ç‡")
        lines.append("| " + " | ".join(headers) + " |")
        lines.append("| " + " | ".join(["---"] * len(headers)) + " |")
        for domain_name, stats in domain_stats.items():
            avg_time = stats['total_time'] / stats['total_questions'] if stats['total_questions'] > 0 else 0
            row = [
                domain_name,
                str(stats['total_questions']),
                f"{stats['total_time']:.2f}",
                f"{avg_time:.2f}",
            ]
            for topk in topk_values:
                correct = stats['topk_stats'][topk]['correct']
                total = stats['topk_stats'][topk]['total']
                acc = (correct / total) if total > 0 else 0
                row.append(f"{correct}/{total}")
                row.append(f"{acc*100:.2f}%")
            lines.append("| " + " | ".join(row) + " |")
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("\n".join(lines))

def aggregate_and_print_summary(domain_stats_runs, topk_values, system_name, repeats, model_name, summary_file):
    """Aggregates results from multiple runs and prints/saves a summary."""
    import statistics
    import io
    
    domains = list(domain_stats_runs[0].keys())
    # Handle key differences: some use 'total_questions', some 'total_samples' (I unified to total_questions in BaseQASystem but let's be safe)
    total_questions_runs = [sum(d.get('total_questions', d.get('total_samples', 0)) for d in r.values()) for r in domain_stats_runs]
    total_time_runs = [sum(d['total_time'] for d in r.values()) for r in domain_stats_runs]
    
    topk_acc_runs = {k: [] for k in topk_values}
    for r in domain_stats_runs:
        for k in topk_values:
            corr = sum(d['topk_stats'][k]['correct'] for d in r.values())
            tot = sum(d['topk_stats'][k]['total'] for d in r.values())
            acc = (corr / tot) if tot > 0 else 0.0
            topk_acc_runs[k].append(acc)
            
    mq = statistics.mean(total_questions_runs)
    mt = statistics.mean(total_time_runs)
    st = statistics.stdev(total_time_runs) if len(total_time_runs) > 1 else 0.0
    
    avg_list = [t / q if q > 0 else 0.0 for t, q in zip(total_time_runs, total_questions_runs)]
    ma = statistics.mean(avg_list)
    sa = statistics.stdev(avg_list) if len(avg_list) > 1 else 0.0
    
    # Capture stats to string for markdown
    stats_buffer = io.StringIO()
    def log_stats(msg):
        print(msg) # To console
        print(msg, file=stats_buffer) # To buffer
    
    log_stats(f"\n{'='*60}")
    log_stats(f"ğŸ“Š {system_name}è¯„ä¼°ç»“æœç»Ÿè®¡ï¼ˆ{repeats}æ¬¡å¹³å‡Â±æ ‡å‡†å·®ï¼‰")
    log_stats(f"{'='*60}")
    log_stats(f"Embeddingæ¨¡å‹/ç¼–ç å™¨: {model_name}")
    log_stats(f"æ€»é—®é¢˜æ•°: {int(mq)}")
    log_stats(f"æ€»è€—æ—¶: {mt:.2f}sÂ±{st:.2f}s")
    log_stats(f"å¹³å‡æ¯é¢˜è€—æ—¶: {ma:.2f}sÂ±{sa:.2f}s")
    log_stats(f"\nğŸ¯ æ€»ä½“å‡†ç¡®ç‡:")
    for k in topk_values:
        macc = statistics.mean(topk_acc_runs[k])
        sacc = statistics.stdev(topk_acc_runs[k]) if len(topk_acc_runs[k]) > 1 else 0.0
        log_stats(f"  TopK-{k}: {macc*100:.2f}%Â±{sacc*100:.2f}%")
        
    log_stats(f"\nğŸ“‹ å„é¢†åŸŸè¯¦ç»†ç»Ÿè®¡:")
    for domain in domains:
        tts = [r[domain]['total_time'] for r in domain_stats_runs]
        tqs = [r[domain].get('total_questions', r[domain].get('total_samples', 0)) for r in domain_stats_runs]
        ats = [t / q if q > 0 else 0.0 for t, q in zip(tts, tqs)]
        
        mt_domain = statistics.mean(tts)
        st_domain = statistics.stdev(tts) if len(tts) > 1 else 0.0
        ma_domain = statistics.mean(ats)
        sa_domain = statistics.stdev(ats) if len(ats) > 1 else 0.0
        
        log_stats(f"  {domain}:")
        log_stats(f"    é—®é¢˜æ•°: {int(statistics.mean(tqs))}")
        log_stats(f"    æ€»è€—æ—¶: {mt_domain:.2f}sÂ±{st_domain:.2f}s")
        log_stats(f"    å¹³å‡è€—æ—¶: {ma_domain:.2f}sÂ±{sa_domain:.2f}s")
        for k in topk_values:
            accs = []
            for r in domain_stats_runs:
                c = r[domain]['topk_stats'][k]['correct']
                t = r[domain]['topk_stats'][k]['total']
                accs.append((c / t) if t > 0 else 0.0)
            macc_d = statistics.mean(accs)
            sacc_d = statistics.stdev(accs) if len(accs) > 1 else 0.0
            log_stats(f"    TopK-{k}: {macc_d*100:.2f}%Â±{sacc_d*100:.2f}%")
    
    # Append to summary file
    with open(summary_file, 'a', encoding='utf-8') as f: # Use 'a' to append
        f.write(stats_buffer.getvalue())
    print(f"\nğŸ’¾ ç»“æœå·²è¿½åŠ åˆ°Markdown: {summary_file}")


class StandardQASystem(BaseQASystem):
    def __init__(self, dataset_path, embedding_model_path, device_id="0", system_name="Standard"):
        super().__init__(dataset_path, device_id, system_name)
        # åˆå§‹åŒ–embeddingæ¨¡å‹
        self.embedding_model = EmbeddingModel(embedding_model_path)

    def process_single_question(self, sample, topk_values, verbose=True):
        """å¤„ç†å•ä¸ªé—®é¢˜ï¼šç¼–ç ä¸€æ¬¡ï¼Œæ£€ç´¢ä¸€æ¬¡ï¼Œç„¶åå¯¹ä¸åŒtopkè¿›è¡Œé—®ç­”"""
        question = sample['question']
        question_id = sample['_id']
        domain = sample['sub_domain'].lower()
        choices = {
            'A': sample['choice_A'],
            'B': sample['choice_B'], 
            'C': sample['choice_C'],
            'D': sample['choice_D']
        }
        chunks = sample['chunks']
        
        if verbose:
            print(f"\nğŸ“ é—®é¢˜ID: {question_id}")
            print(f"ğŸ“ é—®é¢˜: {question[:100]}...")
            print(f"ğŸ·ï¸ é¢†åŸŸ: {domain}")
            print(f"ğŸ“¦ å¯ç”¨chunks: {len(chunks)}ä¸ª")
        
        # 1. é—®é¢˜ç¼–ç ï¼ˆåªç¼–ç ä¸€æ¬¡ï¼‰
        if verbose:
            print(f"    ğŸ” å¼€å§‹ç¼–ç é—®é¢˜...")
        question_embedding = self.embedding_model.encode([question], batch_size=1)
        
        # 2. ç¼–ç æ‰€æœ‰chunks
        if verbose:
            print(f"    ğŸ“¦ ç¼–ç  {len(chunks)} ä¸ªchunks...")
        start_time = time.perf_counter()
        chunk_embeddings = self.embedding_model.encode(chunks)
        encoding_time = time.perf_counter() - start_time
        
        # è®°å½•chunksç¼–ç ç»Ÿè®¡ï¼ˆåˆå¹¶åˆ†å—æ—¶é—´ä¸ç¼–ç æ—¶é—´ï¼‰
        # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾sampleä¸­æœ‰'time'å­—æ®µè¡¨ç¤ºåˆ†å—æ—¶é—´
        total_time = float(sample.get('time', 0.0)) + float(encoding_time)
        
        chunks_encoding_stats = {
            'question_id': question_id,
            'time': total_time
        }
        
        # 3. è®¡ç®—ä¸€æ¬¡ç›¸ä¼¼åº¦ä¸æ’åº
        similarities = cosine_similarity(question_embedding, chunk_embeddings)[0]
        sorted_indices = np.argsort(similarities)[::-1]

        # 4. å¯¹æ¯ä¸ªtopkè¿›è¡Œé—®ç­”
        results = []
        for topk in topk_values:
            topk_indices = sorted_indices[:topk]
            relevant_chunks = [chunks[idx] for idx in topk_indices]
            similarity_scores = [float(similarities[idx]) for idx in topk_indices]

            context = truncate_chunks_by_topk(self.llm.tokenizer, relevant_chunks)
            prompt = build_mcq_prompt(context, question, choices)

            response = self.llm.chat(prompt)
            parsed_response = self.parse_json_response(response)
            if isinstance(parsed_response, dict):
                model_answer = parsed_response.get('answer', 'UNKNOWN')
            else:
                model_answer = 'UNKNOWN'

            original_chunking_stats = {
                'chunking_time': sample.get('time', 0.0)
            }

            result = {
                'question_id': question_id,
                'question': question,
                'domain': domain,
                'topk': topk,
                'retrieved_chunks': [{'text': chunk, 'similarity': sim} for chunk, sim in zip(relevant_chunks, similarity_scores)],
                'original_chunking_stats': original_chunking_stats,
                'chunks_encoding_stats': chunks_encoding_stats,
                'raw_response': response,
                'parsed_response': parsed_response,
                'model_answer': model_answer,
                'correct_answer': sample['answer'],
                'choices': choices,
                'is_correct': model_answer.upper() == sample['answer'].upper()
            }

            results.append(result)
            if verbose:
                print(f"      TopK={topk}: {'âœ… æ­£ç¡®' if result['is_correct'] else 'âŒ é”™è¯¯'} (ç­”æ¡ˆ: {model_answer} / æ­£ç¡®: {sample['answer']})")

        return results

from src.encoder import UnifiedEncoder

class FreeChunkerQASystem(BaseQASystem):
    def __init__(self, dataset_path, encoder_model_name, encoder_model_path, device_id="7", system_name="FreeChunker"):
        # Initialize base without loading embedding model locally in the same way
        super().__init__(dataset_path, device_id, system_name)
        
        # åˆå§‹åŒ–ç»Ÿä¸€ç¼–ç å™¨
        print(f"ğŸ”§ æ­£åœ¨åˆå§‹åŒ–ç»Ÿä¸€ç¼–ç å™¨: {encoder_model_name}")
        self.encoder_model_name = encoder_model_name
        self.encoder = UnifiedEncoder(
            model_name=encoder_model_name,
            local_model_path=encoder_model_path
        )
        
    def process_single_question(self, sample, topk_values, verbose=True):
        """å¤„ç†å•ä¸ªé—®é¢˜ï¼šç¼–ç ä¸€æ¬¡ï¼Œæ£€ç´¢ä¸€æ¬¡ï¼Œç„¶åå¯¹ä¸åŒtopkè¿›è¡Œç¡®å®šæ€§é—®ç­”"""
        question = sample['question']
        question_id = sample['_id']
        domain = sample['sub_domain'].lower()
        choices = {
            'A': sample['choice_A'],
            'B': sample['choice_B'], 
            'C': sample['choice_C'],
            'D': sample['choice_D']
        }
        context = sample['context']
        
        if verbose:
            print(f"\nğŸ“ é—®é¢˜ID: {question_id}")
            print(f"ğŸ“ é—®é¢˜: {question[:100]}...")
            print(f"ğŸ·ï¸ é¢†åŸŸ: {domain}")
        
        start_time = time.perf_counter()
        self.encoder.build_vector_store(context, show_progress=False)
        run_time = time.perf_counter() - start_time
        
        # è®°å½•chunksç¼–ç ç»Ÿè®¡ï¼ˆä»…æ—¶é—´ï¼‰
        chunks_encoding_stats = {
            'question_id': question_id,
            'encoding_time': run_time
        }
        
        # è·å–åŸå§‹åˆ†å—æ—¶é—´
        original_chunking_stats = {
            'chunking_time': sample.get('time', 0.0)
        }
        
        # 2. å¯¹æ¯ä¸ªtopkå€¼åˆ†åˆ«è¿›è¡Œæ£€ç´¢å’ŒLLMé—®ç­”
        results = []
        for topk in topk_values:
            # å¯¹å½“å‰topkå€¼è¿›è¡Œæ£€ç´¢
            retrieved_context = self.encoder.query(
                query=question,
                top_k=topk,
                aggregation_mode='post',
                tokenizer=self.llm.tokenizer
            )
            
            prompt = build_mcq_prompt(retrieved_context, question, choices)

            # è°ƒç”¨LLMç”Ÿæˆç­”æ¡ˆï¼ˆç¡®å®šæ€§è¿è¡Œï¼‰
            response = self.llm.chat(prompt)
            
            # è§£æJSONå“åº”
            parsed_response = self.parse_json_response(response)
            
            if isinstance(parsed_response, dict):
                model_answer = parsed_response.get('answer', 'UNKNOWN')
            else:
                model_answer = 'UNKNOWN'
            
            # ä¿å­˜ç»“æœ
            result = {
                'question_id': question_id,
                'question': question,
                'domain': domain,
                'topk': topk,
                'retrieved_context': retrieved_context,
                'original_chunking_stats': original_chunking_stats,
                'chunks_encoding_stats': chunks_encoding_stats,
                'raw_response': response,
                'parsed_response': parsed_response,
                'model_answer': model_answer,
                'correct_answer': sample['answer'],
                'choices': choices,
                'is_correct': model_answer.upper() == sample['answer'].upper()
            }
            
            results.append(result)
            
            # æ˜¾ç¤ºæ¯ä¸ªå›ç­”çš„å¯¹é”™
            if verbose:
                print(f"      TopK={topk}: {'âœ… æ­£ç¡®' if result['is_correct'] else 'âŒ é”™è¯¯'} (ç­”æ¡ˆ: {model_answer} / æ­£ç¡®: {sample['answer']})")
        
        return results

