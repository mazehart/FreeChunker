
import torch
import numpy as np
from datasets import load_from_disk
from sklearn.metrics.pairwise import cosine_similarity
import json
import re
from datetime import datetime
import json_repair
from utils.monitor import Monitor
from openai import OpenAI
import requests
import time
from transformers import AutoTokenizer
from tqdm import tqdm
import os
import io
import sys
import statistics

chunk_size = 512

class EmbeddingModel:
    def __init__(self, model_path):
        from sentence_transformers import SentenceTransformer
        print(f"ğŸ”§ æ­£åœ¨åŠ è½½embeddingæ¨¡å‹: {model_path}")
        
        # ä½¿ç”¨sentence-transformersï¼Œè‡ªåŠ¨å¤„ç†æ‰¹æ¬¡å’Œæ˜¾å­˜ç®¡ç†
        self.model = SentenceTransformer(model_path, trust_remote_code=True)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
        self.model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œç¦ç”¨dropoutç­‰è®­ç»ƒç›¸å…³å±‚
        
        print(f"âœ… Embeddingæ¨¡å‹åŠ è½½å®Œæˆï¼Œè®¾å¤‡: {self.device}")
        print(f"ğŸ“ æ¨¡å‹æœ€å¤§åºåˆ—é•¿åº¦: {self.model.max_seq_length}")
        print(f"ğŸ¯ æ¨¡å‹ç»´åº¦: {self.model.get_sentence_embedding_dimension()}")
    
    def encode(self, texts, batch_size=6):
        """ç¼–ç æ–‡æœ¬"""
        embeddings = self.model.encode(
            texts,
            batch_size=batch_size,
            show_progress_bar=False,
            convert_to_numpy=True,
            normalize_embeddings=True  # å½’ä¸€åŒ–embedding
        )
        
        return embeddings

class VLLMClient:
    def __init__(self, system_prompt="You are an excellent reading comprehension assistant. Please provide answers in JSON format."):
        """ç›´æ¥ä½¿ç”¨ OpenAI å®¢æˆ·ç«¯è¿æ¥ vLLM"""
        self.client = OpenAI(
            api_key="EMPTY",
            base_url="http://localhost:8888/v1"
        )
        self.system_prompt = system_prompt
        self.device = "vllm-server"
        
        # åŠ è½½ Qwen tokenizer ç”¨äºç²¾ç¡®è®¡ç®— token æ•°é‡
        print("ğŸ”§ æ­£åœ¨åŠ è½½ Qwen tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained("/share/home/ecnuzwx/UnifiedRAG/cache/models--Qwen--Qwen3-8B")
        self.max_context_length = 40000  # Qwen3-8B çš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
        self.reserved_tokens = 1000  # ä¸ºç³»ç»Ÿæç¤ºã€é—®é¢˜å’Œå›ç­”é¢„ç•™çš„ token æ•°é‡
        
        print(f"âœ… vLLM å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ² ç”Ÿæˆå‚æ•°: ç¡®å®šæ€§è¾“å‡ºæ¨¡å¼")
        print(f"ğŸ“ æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦: {self.max_context_length}, é¢„ç•™ token: {self.reserved_tokens}")
    
    def chat(self, text, **kwargs):
        """ç”Ÿæˆå›ç­”"""
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": text}
        ]
        
        request_params = {
            "model": "/share/home/ecnuzwx/UnifiedRAG/cache/models--Qwen--Qwen3-8B",
            "messages": messages,
            "max_tokens": 100,
            "temperature": 0.0,  # ç¡®å®šæ€§è¾“å‡º
            "extra_body": {
                "do_sample": False,  # ç¡®å®šæ€§è¾“å‡º
                "chat_template_kwargs": {"enable_thinking": False}
            }
        }
        
        response = self.client.chat.completions.create(**request_params)
        return response.choices[0].message.content.strip()
    
    def truncate_retrieved_context(self, retrieved_context, question):
        """æˆªæ–­æ£€ç´¢å†…å®¹ä»¥ç¡®ä¿ä¸è¶…è¿‡æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦"""
        # è®¡ç®—ç³»ç»Ÿæç¤ºå’Œé—®é¢˜çš„ token æ•°é‡
        system_tokens = len(self.tokenizer.encode(self.system_prompt))
        question_tokens = len(self.tokenizer.encode(question))
        
        # è®¡ç®—å¯ç”¨äºæ£€ç´¢å†…å®¹çš„æœ€å¤§ token æ•°é‡
        max_context_tokens = self.max_context_length - system_tokens - question_tokens - self.reserved_tokens
        
        # å¦‚æœæ£€ç´¢å†…å®¹çš„ token æ•°é‡è¶…è¿‡é™åˆ¶ï¼Œåˆ™è¿›è¡Œæˆªæ–­
        context_tokens = self.tokenizer.encode(retrieved_context)
        if len(context_tokens) > max_context_tokens:
            print(f"âš ï¸  æ£€ç´¢å†…å®¹è¿‡é•¿ ({len(context_tokens)} tokens)ï¼Œæˆªæ–­è‡³ {max_context_tokens} tokens")
            truncated_tokens = context_tokens[:max_context_tokens]
            retrieved_context = self.tokenizer.decode(truncated_tokens, skip_special_tokens=True)
        
        return retrieved_context

class QASystem:
    def __init__(self, 
                 semantic_dataset_path='./datasets_chunked/Semantic',
                 embedding_model_path='/share/home/ecnuzwx/UnifiedRAG/cache/models--jinaai--jina-embeddings-v2-small-en'):
        
        print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–Semanticé—®ç­”ç³»ç»Ÿ...")
        
        # åŠ è½½æ•°æ®é›†
        print("ğŸ“Š æ­£åœ¨åŠ è½½Semanticæ•°æ®é›†...")
        self.datasets = load_from_disk(semantic_dataset_path)
        print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆï¼ŒåŒ…å«é¢†åŸŸ: {list(self.datasets.keys())}")
        
        # åˆå§‹åŒ–ç›‘æ§å™¨
        self.monitor = Monitor(device_id="3")
        self.monitor.setup()
        
        # åˆå§‹åŒ–embeddingæ¨¡å‹
        self.embedding_model = EmbeddingModel(embedding_model_path)
        
        # åˆå§‹åŒ– vLLM å®¢æˆ·ç«¯
        print(f"ğŸ”§ æ­£åœ¨åˆå§‹åŒ–LLM (vLLM API æ¨¡å¼)...")
        
        # æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€
        if not self._check_vllm_server():
            raise RuntimeError("âŒ vLLM æœåŠ¡å™¨ä¸å¯ç”¨ï¼è¯·å…ˆå¯åŠ¨ server.py æä¾›çš„æœåŠ¡")
        
        try:
            self.llm = VLLMClient()
            print("âœ… vLLM API å®¢æˆ·ç«¯è¿æ¥æˆåŠŸ")
        except Exception as e:
            raise RuntimeError(f"âŒ vLLM è¿æ¥å¤±è´¥: {e}")
        
        print("âœ… Semanticé—®ç­”ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def _check_vllm_server(self):
        """æ£€æŸ¥ vLLM æœåŠ¡å™¨çŠ¶æ€"""
        try:
            response = requests.get("http://localhost:8888/health", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def parse_json_response(self, response):
        """è§£æLLMè¿”å›çš„JSONå“åº”"""
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            json_str = json_match.group()
            try:
                parsed = json_repair.loads(json_str)
                return parsed
            except:
                pass
        
        try:
            parsed = json_repair.loads(response)
            return parsed
        except:
            return {"answer": "UNKNOWN"}
    
    def process_single_question(self, sample, topk_values, verbose=True):
        """å¤„ç†å•ä¸ªé—®é¢˜ï¼šç¼–ç ä¸€æ¬¡ï¼Œæ£€ç´¢ä¸€æ¬¡ï¼Œç„¶åå¯¹ä¸åŒtopkè¿›è¡Œé—®ç­”ï¼ˆå•æ¬¡ç¡®å®šæ€§è¿è¡Œï¼‰"""
        question = sample['question']
        question_id = sample['_id']
        domain = sample['sub_domain'].lower()
        choices = {
            'A': sample['choice_A'],
            'B': sample['choice_B'], 
            'C': sample['choice_C'],
            'D': sample['choice_D']
        }
        chunks = sample['chunks']
        
        if verbose:
            print(f"\nğŸ“ é—®é¢˜ID: {question_id}")
            print(f"ğŸ“ é—®é¢˜: {question[:100]}...")
            print(f"ğŸ·ï¸ é¢†åŸŸ: {domain}")
            print(f"ğŸ“¦ å¯ç”¨chunks: {len(chunks)}ä¸ª")
        
        # 1. é—®é¢˜ç¼–ç ï¼ˆåªç¼–ç ä¸€æ¬¡ï¼‰
        if verbose:
            print(f"    ğŸ” å¼€å§‹ç¼–ç é—®é¢˜...")
        question_embedding = self.embedding_model.encode([question], batch_size=1)
        
        # 2. ç¼–ç æ‰€æœ‰chunksï¼ˆè®°å½•èµ„æºä½¿ç”¨ï¼‰
        if verbose:
            print(f"    ğŸ“¦ ç¼–ç  {len(chunks)} ä¸ªchunks...")
        start_time = time.perf_counter()
        chunk_embeddings = self.embedding_model.encode(chunks)
        encoding_time = time.perf_counter() - start_time
        
        # è®°å½•chunksç¼–ç ç»Ÿè®¡ï¼ˆåˆå¹¶åˆ†å—æ—¶é—´ä¸ç¼–ç æ—¶é—´ï¼‰
        total_time = float(sample.get('time', 0.0)) + float(encoding_time)
        if verbose:
            print(f"    âœ… Chunksç¼–ç å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}s (åˆ†å—+ç¼–ç )")
        
        # è®°å½•chunksç¼–ç ç»Ÿè®¡
        chunks_encoding_stats = {
            'question_id': question_id,
            'time': total_time
        }
        
        # 3. è®¡ç®—ä¸€æ¬¡ç›¸ä¼¼åº¦ä¸æ’åº
        similarities = cosine_similarity(question_embedding, chunk_embeddings)[0]
        sorted_indices = np.argsort(similarities)[::-1]

        # 4. å¯¹æ¯ä¸ªtopkè¿›è¡Œé—®ç­”ï¼ˆå•æ¬¡ç¡®å®šæ€§è¿è¡Œï¼‰
        results = []
        for topk in topk_values:
            topk_indices = sorted_indices[:topk]
            relevant_chunks = [chunks[idx] for idx in topk_indices]
            similarity_scores = [float(similarities[idx]) for idx in topk_indices]

            context = "\n\n".join(relevant_chunks)
            
            # æˆªæ–­æ£€ç´¢å†…å®¹ä»¥ç¡®ä¿ä¸è¶…è¿‡æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
            context = self.llm.truncate_retrieved_context(context, question)
            
            prompt = f"""Based on the following document content, please answer the multiple choice question.

Document Content:
{context}

Question: {question}

Options:
A. {choices['A']}
B. {choices['B']}
C. {choices['C']}
D. {choices['D']}

Please carefully analyze the document content and select the correct answer. Respond in JSON format with the following structure:
{{
    "answer": "A/B/C/D"
}}"""

            response = self.llm.chat(prompt)
            parsed_response = self.parse_json_response(response)
            if isinstance(parsed_response, dict):
                model_answer = parsed_response.get('answer', 'UNKNOWN')
            else:
                model_answer = 'UNKNOWN'

            original_chunking_stats = {
                'chunking_time': sample.get('time', 0.0)
            }

            result = {
                'question_id': question_id,
                'question': question,
                'domain': domain,
                'topk': topk,
                'retrieved_chunks': [{'text': chunk, 'similarity': sim} for chunk, sim in zip(relevant_chunks, similarity_scores)],
                'original_chunking_stats': original_chunking_stats,
                'chunks_encoding_stats': chunks_encoding_stats,
                'raw_response': response,
                'parsed_response': parsed_response,
                'model_answer': model_answer,
                'correct_answer': sample['answer'],
                'choices': choices,
                'is_correct': model_answer.upper() == sample['answer'].upper()
            }

            results.append(result)
            if verbose:
                print(f"      TopK={topk}: {'âœ… æ­£ç¡®' if result['is_correct'] else 'âŒ é”™è¯¯'} (ç­”æ¡ˆ: {model_answer} / æ­£ç¡®: {sample['answer']})")

        return results
    
    def evaluate_single_run(self, topk_values=[1, 3, 5], verbose=False, save_results=True):
        """å•æ¬¡è¯„ä¼°è¿è¡Œ - æµ‹è¯•æ‰€æœ‰æ ·æœ¬ï¼ˆç¡®å®šæ€§è¾“å‡ºï¼‰"""
        print("ğŸ¯ ç¡®å®šæ€§è¯„ä¼°æ¨¡å¼ (do_sample=False, temperature=0.0)")
        print(f"ğŸ¯ TopKå€¼åˆ—è¡¨: {topk_values}")
        
        # å­˜å‚¨æ‰€æœ‰ç»“æœ
        all_results = []
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_questions = 0
        total_time = 0
        domain_stats = {}
        
        # æŒ‰é¢†åŸŸå¤„ç†
        for domain_name in self.datasets:
            if verbose:
                print(f"\nğŸ“‹ å¤„ç†é¢†åŸŸ: {domain_name}")
            
            domain_dataset = self.datasets[domain_name]
            total_samples = len(domain_dataset)
            total_questions += total_samples
            
            print(f"    å¤„ç†{domain_name}: {total_samples}ä¸ªæ ·æœ¬")
            
            # åˆå§‹åŒ–é¢†åŸŸç»Ÿè®¡
            domain_stats[domain_name] = {
                'total_samples': total_samples,
                'total_time': 0,
                'topk_stats': {topk: {'correct': 0, 'total': 0} for topk in topk_values}
            }
            
            # å¤„ç†è¯¥é¢†åŸŸçš„æ‰€æœ‰æ ·æœ¬
            for i in tqdm(range(total_samples), desc=f"Processing {domain_name}", unit="sample"):
                sample = domain_dataset[i]
                
                # å¯¹è¯¥æ ·æœ¬è¿›è¡Œä¸€æ¬¡ç¼–ç ï¼Œå¤šæ¬¡æ£€ç´¢
                sample_results = self.process_single_question(sample, topk_values, verbose=False)
                
                # è®¡ç®—è¯¥æ ·æœ¬çš„æ—¶é—´ï¼ˆä»chunks_encoding_statsä¸­è·å–ï¼‰
                if sample_results:
                    sample_time = sample_results[0].get('chunks_encoding_stats', {}).get('time', 0.0)
                    total_time += sample_time
                    domain_stats[domain_name]['total_time'] += sample_time
                
                # å°†ç»“æœç›´æ¥æ·»åŠ åˆ°æ€»ç»“æœä¸­
                all_results.extend(sample_results)
                
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                for result in sample_results:
                    topk = result['topk']
                    is_correct = result['is_correct']
                    domain_stats[domain_name]['topk_stats'][topk]['total'] += 1
                    if is_correct:
                        domain_stats[domain_name]['topk_stats'][topk]['correct'] += 1
            
            print(f"  âœ… {domain_name} å®Œæˆ: {total_samples}/{total_samples} (100%)")
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        print(f"\næ€»é—®é¢˜æ•°: {total_questions}")
        print(f"æ€»è€—æ—¶: {total_time:.2f}s")
        print(f"å¹³å‡æ¯é¢˜è€—æ—¶: {total_time/total_questions:.2f}s")
        
        # è®¡ç®—æ€»ä½“å‡†ç¡®ç‡
        overall_stats = {topk: {'correct': 0, 'total': 0} for topk in topk_values}
        
        for domain_name, stats in domain_stats.items():
            for topk in topk_values:
                correct = stats['topk_stats'][topk]['correct']
                total = stats['topk_stats'][topk]['total']
                
                # ç´¯è®¡åˆ°æ€»ä½“ç»Ÿè®¡
                overall_stats[topk]['correct'] += correct
                overall_stats[topk]['total'] += total
        
        # æ˜¾ç¤ºæ€»ä½“å‡†ç¡®ç‡
        print(f"\nğŸ¯ æ€»ä½“å‡†ç¡®ç‡:")
        for topk in topk_values:
            correct = overall_stats[topk]['correct']
            total = overall_stats[topk]['total']
            accuracy = correct / total if total > 0 else 0
            print(f"  TopK-{topk}: {accuracy*100:.2f}%Â±0.00% ({correct}/{total})")
        
        # æ˜¾ç¤ºå„é¢†åŸŸè¯¦ç»†ç»Ÿè®¡
        print(f"\nğŸ“‹ å„é¢†åŸŸè¯¦ç»†ç»Ÿè®¡:")
        for domain_name, stats in domain_stats.items():
            print(f"  {domain_name}:")
            print(f"    é—®é¢˜æ•°: {stats['total_samples']}")
            print(f"    æ€»è€—æ—¶: {stats['total_time']:.2f}s")
            print(f"    å¹³å‡è€—æ—¶: {stats['total_time']/stats['total_samples']:.2f}s")
            for topk in topk_values:
                correct = stats['topk_stats'][topk]['correct']
                total = stats['topk_stats'][topk]['total']
                accuracy = correct / total if total > 0 else 0
                print(f"    TopK-{topk}: {accuracy*100:.2f}% ({correct}/{total})")
        
        # ä¿å­˜ç»“æœ
        if save_results:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            results_file = f"semantic_qa_results_{timestamp}.md"
            lines = []
            lines.append(f"# Semanticé—®ç­”ç»“æœ ({timestamp})")
            lines.append("")
            lines.append("## æ€»ä½“å‡†ç¡®ç‡")
            lines.append("")
            lines.append("| TopK | Correct | Total | Accuracy |")
            lines.append("| --- | ---: | ---: | ---: |")
            for topk in topk_values:
                correct = overall_stats[topk]['correct']
                total = overall_stats[topk]['total']
                accuracy = correct / total if total > 0 else 0
                lines.append(f"| {topk} | {correct} | {total} | {accuracy*100:.2f}% |")
            lines.append("")
            lines.append("## å„é¢†åŸŸè¯¦ç»†ç»Ÿè®¡")
            headers = ["é¢†åŸŸ", "é—®é¢˜æ•°", "æ€»è€—æ—¶(s)", "å¹³å‡è€—æ—¶(s)"]
            for topk in topk_values:
                headers.append(f"TopK-{topk} æ­£ç¡®/æ€»æ•°")
                headers.append(f"TopK-{topk} å‡†ç¡®ç‡")
            lines.append("| " + " | ".join(headers) + " |")
            lines.append("| " + " | ".join(["---"] * len(headers)) + " |")
            for domain_name, stats in domain_stats.items():
                avg_time = stats['total_time'] / stats['total_samples'] if stats['total_samples'] > 0 else 0
                row = [
                    domain_name,
                    str(stats['total_samples']),
                    f"{stats['total_time']:.2f}",
                    f"{avg_time:.2f}",
                ]
                for topk in topk_values:
                    correct = stats['topk_stats'][topk]['correct']
                    total = stats['topk_stats'][topk]['total']
                    acc = (correct / total) if total > 0 else 0
                    row.append(f"{correct}/{total}")
                    row.append(f"{acc*100:.2f}%")
                lines.append("| " + " | ".join(row) + " |")
            with open(results_file, 'w', encoding='utf-8') as f:
                f.write("\n".join(lines))
            print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜ä¸ºMarkdownè¡¨æ ¼: {results_file}")
        
        return all_results, domain_stats



def check_vllm_server():
    """æ£€æŸ¥ vLLM æœåŠ¡å™¨æ˜¯å¦å¯ç”¨"""
    print("ğŸ” æ£€æŸ¥ vLLM æœåŠ¡å™¨çŠ¶æ€...")
    try:
        response = requests.get("http://localhost:8888/health", timeout=5)
        if response.status_code == 200:
            print("âœ… vLLM æœåŠ¡å™¨è¿è¡Œæ­£å¸¸")
            return True
    except:
        pass
    
    print("âŒ vLLM æœåŠ¡å™¨ä¸å¯ç”¨ï¼")
    print("ğŸ“‹ è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤å¯åŠ¨æœåŠ¡å™¨ï¼š")
    print("   1. è¿è¡Œ: sbatch server.sh")
    print("   2. ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨å®Œæˆ")
    print("   3. æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€: curl http://localhost:8888/health")
    raise RuntimeError("vLLM æœåŠ¡å™¨ä¸å¯ç”¨ï¼Œè¯·å…ˆå¯åŠ¨ server.py")

def main():
    print("ğŸ¯ Semanticé—®ç­”ç³»ç»Ÿ - ç¡®å®šæ€§è¯„ä¼°")
    print("=" * 60)
    check_vllm_server()
    topk_values = [5, 10]
    embed_models = [
        "/share/home/ecnuzwx/UnifiedRAG/cache/models--BAAI--bge-m3",
        "/share/home/ecnuzwx/UnifiedRAG/cache/models--jinaai--jina-embeddings-v2-small-en",
        "/share/home/ecnuzwx/UnifiedRAG/cache/models--nomic-ai--nomic-embed-text-v1.5",
    ]
    repeats = 3
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    summary_file = f"semantic_qa_summary_{timestamp}.md"
    markdown_buffer = io.StringIO()
    
    for embed_model in embed_models:
        print("\nğŸš€ åˆå§‹åŒ–ç³»ç»Ÿ...")
        print(f"\nğŸ“‹ è¯„ä¼°é…ç½®:")
        print(f"   TopKå€¼: {topk_values}")
        print(f"   åˆ†å—æ–¹æ³•: Semantic")
        print(f"   Embeddingæ¨¡å‹: {embed_model}")
        domain_stats_runs = []
        for _ in range(repeats):
            semantic_dataset_path = "/share/home/ecnuzwx/UnifiedRAG/LongBench-v2_chunked/Semantic/models--BAAI--bge-m3"
            qa_system = QASystem(
                semantic_dataset_path=semantic_dataset_path,
                embedding_model_path=embed_model
            )
            results, domain_stats = qa_system.evaluate_single_run(
                topk_values=topk_values,
                verbose=False,
                save_results=False
            )
            domain_stats_runs.append(domain_stats)
        domains = list(domain_stats_runs[0].keys())
        total_questions_runs = [sum(d.get('total_questions', d.get('total_samples', 0)) for d in r.values()) for r in domain_stats_runs]
        total_time_runs = [sum(d['total_time'] for d in r.values()) for r in domain_stats_runs]
        topk_acc_runs = {k: [] for k in topk_values}
        for r in domain_stats_runs:
            for k in topk_values:
                corr = sum(d['topk_stats'][k]['correct'] for d in r.values())
                tot = sum(d['topk_stats'][k]['total'] for d in r.values())
                acc = (corr / tot) if tot > 0 else 0.0
                topk_acc_runs[k].append(acc)
        mq = statistics.mean(total_questions_runs)
        mt = statistics.mean(total_time_runs)
        st = statistics.stdev(total_time_runs) if len(total_time_runs) > 1 else 0.0
        avg_list = [t / q if q > 0 else 0.0 for t, q in zip(total_time_runs, total_questions_runs)]
        ma = statistics.mean(avg_list)
        sa = statistics.stdev(avg_list) if len(avg_list) > 1 else 0.0
        
        # Capture stats to string for markdown
        stats_buffer = io.StringIO()
        def log_stats(msg):
            print(msg) # To console
            print(msg, file=stats_buffer) # To buffer
        
        log_stats(f"\n{'='*60}")
        log_stats(f"ğŸ“Š Semanticè¯„ä¼°ç»“æœç»Ÿè®¡ï¼ˆ{repeats}æ¬¡å¹³å‡Â±æ ‡å‡†å·®ï¼‰")
        log_stats(f"{'='*60}")
        log_stats(f"Embeddingæ¨¡å‹: {embed_model}")
        log_stats(f"æ€»é—®é¢˜æ•°: {int(mq)}")
        log_stats(f"æ€»è€—æ—¶: {mt:.2f}sÂ±{st:.2f}s")
        log_stats(f"å¹³å‡æ¯é¢˜è€—æ—¶: {ma:.2f}sÂ±{sa:.2f}s")
        log_stats(f"\nğŸ¯ æ€»ä½“å‡†ç¡®ç‡:")
        for k in topk_values:
            macc = statistics.mean(topk_acc_runs[k])
            sacc = statistics.stdev(topk_acc_runs[k]) if len(topk_acc_runs[k]) > 1 else 0.0
            log_stats(f"  TopK-{k}: {macc*100:.2f}%Â±{sacc*100:.2f}%")
        log_stats(f"\nğŸ“‹ å„é¢†åŸŸè¯¦ç»†ç»Ÿè®¡:")
        for domain in domains:
            tts = [r[domain]['total_time'] for r in domain_stats_runs]
            tqs = [r[domain].get('total_questions', r[domain].get('total_samples', 0)) for r in domain_stats_runs]
            ats = [t / q if q > 0 else 0.0 for t, q in zip(tts, tqs)]
            mt_domain = statistics.mean(tts)
            st_domain = statistics.stdev(tts) if len(tts) > 1 else 0.0
            ma_domain = statistics.mean(ats)
            sa_domain = statistics.stdev(ats) if len(ats) > 1 else 0.0
            log_stats(f"  {domain}:")
            log_stats(f"    é—®é¢˜æ•°: {int(statistics.mean(tqs))}")
            log_stats(f"    æ€»è€—æ—¶: {mt_domain:.2f}sÂ±{st_domain:.2f}s")
            log_stats(f"    å¹³å‡è€—æ—¶: {ma_domain:.2f}sÂ±{sa_domain:.2f}s")
            for k in topk_values:
                accs = []
                for r in domain_stats_runs:
                    c = r[domain]['topk_stats'][k]['correct']
                    t = r[domain]['topk_stats'][k]['total']
                    accs.append((c / t) if t > 0 else 0.0)
                macc_d = statistics.mean(accs)
                sacc_d = statistics.stdev(accs) if len(accs) > 1 else 0.0
                log_stats(f"    TopK-{k}: {macc_d*100:.2f}%Â±{sacc_d*100:.2f}%")
        
        markdown_buffer.write(stats_buffer.getvalue())
    
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write(markdown_buffer.getvalue())
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜ä¸ºMarkdown: {summary_file}")

if __name__ == "__main__":
    main()
