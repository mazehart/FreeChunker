import torch
import numpy as np
from datasets import load_from_disk
from sklearn.metrics.pairwise import cosine_similarity
import json
import re
from datetime import datetime
import json_repair
from utils.monitor import Monitor
from openai import OpenAI
import requests
import time
from transformers import AutoTokenizer
from tqdm import tqdm
import io
import sys
import statistics

class EmbeddingModel:
    def __init__(self, model_path):
        from sentence_transformers import SentenceTransformer
        print(f"ğŸ”§ æ­£åœ¨åŠ è½½embeddingæ¨¡å‹: {model_path}")
        self.model = SentenceTransformer(model_path, trust_remote_code=True)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
        self.model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œç¦ç”¨dropoutç­‰è®­ç»ƒç›¸å…³å±‚
        print(f"âœ… Embeddingæ¨¡å‹åŠ è½½å®Œæˆï¼Œè®¾å¤‡: {self.device}")
        try:
            # ä»…ç”¨äºä¿¡æ¯å±•ç¤ºï¼Œä¸åšé•¿åº¦æˆªæ–­
            print(f"ğŸ“ æ¨¡å‹æœ€å¤§åºåˆ—é•¿åº¦: {self.model.max_seq_length}")
            print(f"ğŸ¯ æ¨¡å‹ç»´åº¦: {self.model.get_sentence_embedding_dimension()}")
        except Exception:
            pass
    
    def encode(self, texts, batch_size=4):
        
        embeddings = self.model.encode(
            texts,
            batch_size=batch_size,
            show_progress_bar=False,
            convert_to_numpy=True,
            normalize_embeddings=True
        )
        return embeddings

class VLLMClient:
    def __init__(self, system_prompt="You are an excellent reading comprehension assistant. Please provide answers in JSON format."):
        """ç›´æ¥ä½¿ç”¨ OpenAI å®¢æˆ·ç«¯è¿æ¥ vLLM"""
        self.client = OpenAI(
            api_key="EMPTY",
            base_url="http://localhost:8888/v1"
        )
        self.system_prompt = system_prompt
        self.device = "vllm-server"
        
        print(f"âœ… vLLM å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ² ç”Ÿæˆå‚æ•°: ç¡®å®šæ€§è¾“å‡ºæ¨¡å¼")
        print("ğŸ”§ æ­£åœ¨åŠ è½½ Qwen tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained("/share/home/ecnuzwx/UnifiedRAG/cache/models--Qwen--Qwen3-8B")
        self.max_context_length = 40000
        self.reserved_tokens = 1000
        print(f"ğŸ“ æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦: {self.max_context_length}, é¢„ç•™ token: {self.reserved_tokens}")
    
    def chat(self, text, **kwargs):
        """ç”Ÿæˆå›ç­”"""
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": text}
        ]
        
        request_params = {
            "model": "/share/home/ecnuzwx/UnifiedRAG/cache/models--Qwen--Qwen3-8B",
            "messages": messages,
            "max_tokens": 512,
            "temperature": 0.0,
            "extra_body": {
                "do_sample": False,
                "chat_template_kwargs": {"enable_thinking": False}
            }
        }
        
        response = self.client.chat.completions.create(**request_params)
        return response.choices[0].message.content.strip()
    
    def truncate_retrieved_context(self, retrieved_context, question):
        system_tokens = len(self.tokenizer.encode(self.system_prompt))
        question_tokens = len(self.tokenizer.encode(question))
        max_context_tokens = self.max_context_length - system_tokens - question_tokens - self.reserved_tokens
        context_tokens = self.tokenizer.encode(retrieved_context)
        if len(context_tokens) > max_context_tokens:
            truncated_tokens = context_tokens[:max_context_tokens]
            retrieved_context = self.tokenizer.decode(truncated_tokens, skip_special_tokens=True)
        return retrieved_context

class QASystem: 
    def __init__(self, 
                 ppl_dataset_path='./datasets_chunked/PPL',
                 embedding_model_path='/share/home/ecnuzwx/UnifiedRAG/cache/models--jinaai--jina-embeddings-v2-small-en'):
        #/share/home/ecnuzwx/UnifiedRAG/cache/models--nomic-ai--nomic-embed-text-v1.5
        #/share/home/ecnuzwx/UnifiedRAG/cache/models--BAAI--bge-m3
        
        print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–PPLé—®ç­”ç³»ç»Ÿ...")
        
        # åŠ è½½æ•°æ®é›†
        print("ğŸ“Š æ­£åœ¨åŠ è½½PPLæ•°æ®é›†...")
        self.datasets = load_from_disk(ppl_dataset_path)
        print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆï¼ŒåŒ…å«é¢†åŸŸ: {list(self.datasets.keys())}")
        
        # åˆå§‹åŒ–ç›‘æ§å™¨
        self.monitor = Monitor(device_id="3")
        self.monitor.setup()
        
        # åˆå§‹åŒ–embeddingæ¨¡å‹
        self.embedding_model = EmbeddingModel(embedding_model_path)
        
        # åˆå§‹åŒ– vLLM å®¢æˆ·ç«¯
        print(f"ğŸ”§ æ­£åœ¨åˆå§‹åŒ–LLM (vLLM API æ¨¡å¼)...")
        
        # æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€
        if not self._check_vllm_server():
            raise RuntimeError("âŒ vLLM æœåŠ¡å™¨ä¸å¯ç”¨ï¼è¯·å…ˆå¯åŠ¨ server.py æä¾›çš„æœåŠ¡")
        
        try:
            self.llm = VLLMClient()
            print("âœ… vLLM API å®¢æˆ·ç«¯è¿æ¥æˆåŠŸ")
        except Exception as e:
            raise RuntimeError(f"âŒ vLLM è¿æ¥å¤±è´¥: {e}")
        
        print("âœ… PPLé—®ç­”ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def _check_vllm_server(self):
        """æ£€æŸ¥ vLLM æœåŠ¡å™¨çŠ¶æ€"""
        try:
            response = requests.get("http://localhost:8888/health", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def parse_json_response(self, response):
        """è§£æLLMè¿”å›çš„JSONå“åº”"""
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            json_str = json_match.group()
            try:
                parsed = json_repair.loads(json_str)
                return parsed
            except:
                pass
        
        try:
            parsed = json_repair.loads(response)
            return parsed
        except:
            return {"answer": "UNKNOWN"}
    
    def process_single_question(self, sample, topk_values, verbose=True):
        """å¤„ç†å•ä¸ªé—®é¢˜ï¼šç¼–ç ä¸€æ¬¡ï¼Œæ£€ç´¢ä¸€æ¬¡ï¼Œç„¶åå¯¹ä¸åŒtopkè¿›è¡Œé—®ç­”ï¼ˆå•æ¬¡ç¡®å®šæ€§è¿è¡Œï¼‰"""
        question = sample['question']
        question_id = sample['_id']
        domain = sample['sub_domain'].lower()
        choices = {
            'A': sample['choice_A'],
            'B': sample['choice_B'], 
            'C': sample['choice_C'],
            'D': sample['choice_D']
        }
        chunks = sample['chunks']
        
        if verbose:
            print(f"\nğŸ“ é—®é¢˜ID: {question_id}")
            print(f"ğŸ“ é—®é¢˜: {question[:100]}...")
            print(f"ğŸ·ï¸ é¢†åŸŸ: {domain}")
            print(f"ğŸ“¦ å¯ç”¨chunks: {len(chunks)}ä¸ª")
        
        # 1. é—®é¢˜ç¼–ç ï¼ˆåªç¼–ç ä¸€æ¬¡ï¼‰
        if verbose:
            print(f"    ğŸ” å¼€å§‹ç¼–ç é—®é¢˜...")
        question_embedding = self.embedding_model.encode([question], batch_size=1)
        
        # 2. ç¼–ç æ‰€æœ‰chunksï¼ˆè®°å½•æ€»è€—æ—¶ = åˆ†å—æ—¶é—´ + ç¼–ç æ—¶é—´ï¼‰
        if verbose:
            print(f"    ğŸ“¦ ç¼–ç  {len(chunks)} ä¸ªchunks...")
        start_time = time.perf_counter()
        chunk_embeddings = self.embedding_model.encode(chunks)
        encoding_time = time.perf_counter() - start_time
        
        # è®°å½•chunksç¼–ç ç»Ÿè®¡ï¼ˆåˆå¹¶åˆ†å—æ—¶é—´ä¸ç¼–ç æ—¶é—´ï¼‰
        total_time = float(sample.get('time', 0.0)) + float(encoding_time)
        chunks_encoding_stats = {
            'question_id': question_id,
            'time': total_time
        }
        
        if verbose:
            print(f"    âœ… Chunksç¼–ç å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.3f}s (åˆ†å—+ç¼–ç )")
        
        # 3. è®¡ç®—ä¸€æ¬¡ç›¸ä¼¼åº¦ä¸æ’åº
        similarities = cosine_similarity(question_embedding, chunk_embeddings)[0]
        sorted_indices = np.argsort(similarities)[::-1]

        # 4. å¯¹æ¯ä¸ªtopkå’Œseedè¿›è¡Œé—®ç­”
        results = []
        for topk in topk_values:
            topk_indices = sorted_indices[:topk]
            relevant_chunks = [chunks[idx] for idx in topk_indices]
            similarity_scores = [float(similarities[idx]) for idx in topk_indices]

            context = "\n\n".join(relevant_chunks)
            context = self.llm.truncate_retrieved_context(context, question)
            prompt = f"""Based on the following document content, please answer the multiple choice question.

Document Content:
{context}

Question: {question}

Options:
A. {choices['A']}
B. {choices['B']}
C. {choices['C']}
D. {choices['D']}

Please carefully analyze the document content and select the correct answer. Respond in JSON format with the following structure:
{{
    "answer": "A/B/C/D"
}}"""

            response = self.llm.chat(prompt)
            parsed_response = self.parse_json_response(response)
            if isinstance(parsed_response, dict):
                model_answer = parsed_response.get('answer', 'UNKNOWN')
            else:
                model_answer = 'UNKNOWN'

            original_chunking_stats = {
                'chunking_time': sample.get('time', 0.0)
            }

            result = {
                'question_id': question_id,
                'question': question,
                'domain': domain,
                'topk': topk,
                'retrieved_chunks': [{'text': chunk, 'similarity': sim} for chunk, sim in zip(relevant_chunks, similarity_scores)],
                'original_chunking_stats': original_chunking_stats,
                'chunks_encoding_stats': chunks_encoding_stats,
                'raw_response': response,
                'parsed_response': parsed_response,
                'model_answer': model_answer,
                'correct_answer': sample['answer'],
                'choices': choices,
                'is_correct': model_answer.upper() == sample['answer'].upper()
            }

            results.append(result)
            if verbose:
                print(f"      TopK={topk}: {'âœ… æ­£ç¡®' if result['is_correct'] else 'âŒ é”™è¯¯'} (ç­”æ¡ˆ: {model_answer} / æ­£ç¡®: {sample['answer']})")

        return results
    
    def evaluate_single_run(self, topk_values=[1, 3, 5], verbose=True, save_results=False):
        """å•æ¬¡è¯„ä¼°è¿è¡Œ - æµ‹è¯•æ‰€æœ‰æ ·æœ¬"""
        print(f"ğŸ² ç”Ÿæˆå‚æ•°: ç¡®å®šæ€§è¾“å‡ºæ¨¡å¼")
        
        # å­˜å‚¨æ‰€æœ‰ç»“æœ
        all_results = []
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_questions = 0
        total_time = 0.0
        domain_stats = {}
        
        # æŒ‰é¢†åŸŸå¤„ç†
        for domain_name in self.datasets:
            if verbose:
                print(f"\nğŸ“‹ å¤„ç†é¢†åŸŸ: {domain_name}")
            
            domain_dataset = self.datasets[domain_name]
            total_samples = len(domain_dataset)
            
            print(f"    å¤„ç†{domain_name}: {total_samples}ä¸ªæ ·æœ¬")
            
            # åˆå§‹åŒ–é¢†åŸŸç»Ÿè®¡
            domain_stats[domain_name] = {
                'total_questions': total_samples,
                'total_time': 0.0,
                'topk_stats': {topk: {'correct': 0, 'total': 0} for topk in topk_values}
            }
            
            # å¤„ç†è¯¥é¢†åŸŸçš„æ‰€æœ‰æ ·æœ¬
            for i in tqdm(range(total_samples), desc=f"Processing {domain_name}", unit="sample"):
                sample = domain_dataset[i]
                
                # å¯¹è¯¥æ ·æœ¬è¿›è¡Œä¸€æ¬¡ç¼–ç ï¼Œå¤šæ¬¡æ£€ç´¢
                sample_results = self.process_single_question(sample, topk_values, verbose=False)
                
                # å°†ç»“æœç›´æ¥æ·»åŠ åˆ°æ€»ç»“æœä¸­
                all_results.extend(sample_results)
                
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                total_questions += 1
                
                # è®¡ç®—è¯¥æ ·æœ¬çš„æ—¶é—´ï¼ˆä»chunks_encoding_statsä¸­è·å–ï¼‰
                if sample_results:
                    sample_time = sample_results[0].get('chunks_encoding_stats', {}).get('time', 0.0)
                    total_time += sample_time
                    domain_stats[domain_name]['total_time'] += sample_time
                
                # æ›´æ–°æ­£ç¡®ç‡ç»Ÿè®¡
                for result in sample_results:
                    topk = result['topk']
                    is_correct = result['is_correct']
                    domain_stats[domain_name]['topk_stats'][topk]['total'] += 1
                    if is_correct:
                        domain_stats[domain_name]['topk_stats'][topk]['correct'] += 1
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        overall_stats = {
            'total_questions': total_questions,
            'total_time': total_time,
            'avg_time_per_question': total_time / total_questions if total_questions > 0 else 0,
            'topk_accuracy': {}
        }
        
        # è®¡ç®—æ¯ä¸ªTopKçš„æ€»ä½“å‡†ç¡®ç‡
        for topk in topk_values:
            total_correct = sum(domain_stats[domain]['topk_stats'][topk]['correct'] for domain in domain_stats)
            total_count = sum(domain_stats[domain]['topk_stats'][topk]['total'] for domain in domain_stats)
            accuracy = total_correct / total_count if total_count > 0 else 0
            overall_stats['topk_accuracy'][topk] = {
                'correct': total_correct,
                'total': total_count,
                'accuracy': accuracy
            }
        
        # æ‰“å°ç»Ÿè®¡ç»“æœ
        if verbose:
            print(f"\n{'='*60}")
            print(f"ğŸ“Š PPLåˆ†å—è¯„ä¼°ç»“æœç»Ÿè®¡")
            print(f"{'='*60}")
            print(f"æ€»é—®é¢˜æ•°: {total_questions}")
            print(f"æ€»è€—æ—¶: {total_time:.2f}s")
            print(f"å¹³å‡æ¯é¢˜è€—æ—¶: {overall_stats['avg_time_per_question']:.2f}s")
            
            print(f"\nğŸ¯ æ€»ä½“å‡†ç¡®ç‡:")
            for topk in topk_values:
                stats = overall_stats['topk_accuracy'][topk]
                print(f"  TopK-{topk}: {stats['accuracy']:.4f} ({stats['correct']}/{stats['total']})")
            
            print(f"\nğŸ“‹ å„é¢†åŸŸè¯¦ç»†ç»Ÿè®¡:")
            for domain, stats in domain_stats.items():
                print(f"  {domain}:")
                print(f"    é—®é¢˜æ•°: {stats['total_questions']}")
                print(f"    æ€»è€—æ—¶: {stats['total_time']:.2f}s")
                print(f"    å¹³å‡è€—æ—¶: {stats['total_time']/stats['total_questions']:.2f}s")
                for topk in topk_values:
                    topk_stat = stats['topk_stats'][topk]
                    accuracy = topk_stat['correct'] / topk_stat['total'] if topk_stat['total'] > 0 else 0
                    print(f"    TopK-{topk}: {accuracy:.4f} ({topk_stat['correct']}/{topk_stat['total']})")
        
        # ä¿å­˜ç»“æœ
        if save_results:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            results_file = f"ppl_qa_results_{timestamp}.json"
            
            # æ„å»ºå®Œæ•´çš„ç»“æœæ•°æ®
            results_data = {
                'metadata': {
                    'timestamp': timestamp,
                    'method': 'PPL_chunking',
                    'topk_values': topk_values,
                    'total_questions': total_questions,
                    'generation_mode': 'deterministic'
                },
                'overall_stats': overall_stats,
                'domain_stats': domain_stats,
                'detailed_results': all_results
            }
            
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(results_data, f, ensure_ascii=False, indent=2)
            
            print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        
        return all_results, domain_stats



def check_vllm_server():
    """æ£€æŸ¥ vLLM æœåŠ¡å™¨æ˜¯å¦å¯ç”¨"""
    print("ğŸ” æ£€æŸ¥ vLLM æœåŠ¡å™¨çŠ¶æ€...")
    try:
        response = requests.get("http://localhost:8888/health", timeout=5)
        if response.status_code == 200:
            print("âœ… vLLM æœåŠ¡å™¨è¿è¡Œæ­£å¸¸")
            return True
    except:
        pass
    
    print("âŒ vLLM æœåŠ¡å™¨ä¸å¯ç”¨ï¼")
    print("ğŸ“‹ è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤å¯åŠ¨æœåŠ¡å™¨ï¼š")
    print("   1. è¿è¡Œ: sbatch server.sh")
    print("   2. ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨å®Œæˆ")
    print("   3. æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€: curl http://localhost:8888/health")
    return False

def main():
    print("å¼€å§‹PPLåˆ†å—æµ‹è¯•...")
    print(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # æ£€æŸ¥VLLMæœåŠ¡å™¨
    if not check_vllm_server():
        print("VLLMæœåŠ¡å™¨æœªå¯åŠ¨ï¼Œè¯·å…ˆå¯åŠ¨æœåŠ¡å™¨")
        return
    
    # æµ‹è¯•å‚æ•°
    topk_values = [5, 10]
    embed_models = [
        "/share/home/ecnuzwx/UnifiedRAG/cache/models--jinaai--jina-embeddings-v2-small-en",
        "/share/home/ecnuzwx/UnifiedRAG/cache/models--BAAI--bge-m3",
        "/share/home/ecnuzwx/UnifiedRAG/cache/models--nomic-ai--nomic-embed-text-v1.5",
    ]
    
    print(f"æµ‹è¯•å‚æ•°: topk_values={topk_values} (ç¡®å®šæ€§è¿è¡Œ)")
    
    ppl_dataset_path = "/share/home/ecnuzwx/UnifiedRAG/LongBench-v2_chunked/PPL/Qwen2.5-1.5B-Instruct"
    repeats = 3
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    summary_file = f"ppl_qa_summary_{timestamp}.md"
    markdown_buffer = io.StringIO()
    
    for embed_model in embed_models:
        print("\nğŸš€ åˆå§‹åŒ–ç³»ç»Ÿ...")
        print(f"\nğŸ“‹ è¯„ä¼°é…ç½®:")
        print(f"   TopKå€¼: {topk_values}")
        print(f"   åˆ†å—æ–¹æ³•: PPL")
        print(f"   Embeddingæ¨¡å‹: {embed_model}")
        domain_stats_runs = []
        for _ in range(repeats):
            qa_system = QASystem(ppl_dataset_path=ppl_dataset_path, embedding_model_path=embed_model)
            results, domain_stats = qa_system.evaluate_single_run(topk_values, verbose=False, save_results=False)
            domain_stats_runs.append(domain_stats)
        domains = list(domain_stats_runs[0].keys())
        total_questions_runs = []
        total_time_runs = []
        topk_acc_runs = {k: [] for k in topk_values}
        for stats_run in domain_stats_runs:
            tq = sum(d.get('total_questions', d.get('total_samples', 0)) for d in stats_run.values())
            tt = sum(d['total_time'] for d in stats_run.values())
            total_questions_runs.append(tq)
            total_time_runs.append(tt)
            for k in topk_values:
                corr = sum(d['topk_stats'][k]['correct'] for d in stats_run.values())
                tot = sum(d['topk_stats'][k]['total'] for d in stats_run.values())
                acc = (corr / tot) if tot > 0 else 0.0
                topk_acc_runs[k].append(acc)
        mq = statistics.mean(total_questions_runs)
        mt = statistics.mean(total_time_runs)
        st = statistics.stdev(total_time_runs) if len(total_time_runs) > 1 else 0.0
        avg_list = [t / q if q > 0 else 0.0 for t, q in zip(total_time_runs, total_questions_runs)]
        ma = statistics.mean(avg_list)
        sa = statistics.stdev(avg_list) if len(avg_list) > 1 else 0.0
        
        # Capture stats to string for markdown
        stats_buffer = io.StringIO()
        def log_stats(msg):
            print(msg) # To console
            print(msg, file=stats_buffer) # To buffer
            
        log_stats(f"\n{'='*60}")
        log_stats(f"ğŸ“Š PPLåˆ†å—è¯„ä¼°ç»“æœç»Ÿè®¡ï¼ˆ{repeats}æ¬¡å¹³å‡Â±æ ‡å‡†å·®ï¼‰")
        log_stats(f"{'='*60}")
        log_stats(f"Embeddingæ¨¡å‹: {embed_model}")
        log_stats(f"æ€»é—®é¢˜æ•°: {int(mq)}")
        log_stats(f"æ€»è€—æ—¶: {mt:.2f}sÂ±{st:.2f}s")
        log_stats(f"å¹³å‡æ¯é¢˜è€—æ—¶: {ma:.2f}sÂ±{sa:.2f}s")
        log_stats(f"\nğŸ¯ æ€»ä½“å‡†ç¡®ç‡:")
        for k in topk_values:
            macc = statistics.mean(topk_acc_runs[k])
            sacc = statistics.stdev(topk_acc_runs[k]) if len(topk_acc_runs[k]) > 1 else 0.0
            log_stats(f"  TopK-{k}: {macc*100:.2f}%Â±{sacc*100:.2f}%")
        log_stats(f"\nğŸ“‹ å„é¢†åŸŸè¯¦ç»†ç»Ÿè®¡:")
        for domain in domains:
            tts = [r[domain]['total_time'] for r in domain_stats_runs]
            tqs = [r[domain].get('total_questions', r[domain].get('total_samples', 0)) for r in domain_stats_runs]
            ats = [t / q if q > 0 else 0.0 for t, q in zip(tts, tqs)]
            mt_domain = statistics.mean(tts)
            st_domain = statistics.stdev(tts) if len(tts) > 1 else 0.0
            ma_domain = statistics.mean(ats)
            sa_domain = statistics.stdev(ats) if len(ats) > 1 else 0.0
            log_stats(f"  {domain}:")
            log_stats(f"    é—®é¢˜æ•°: {int(statistics.mean(tqs))}")
            log_stats(f"    æ€»è€—æ—¶: {mt_domain:.2f}sÂ±{st_domain:.2f}s")
            log_stats(f"    å¹³å‡è€—æ—¶: {ma_domain:.2f}sÂ±{sa_domain:.2f}s")
            for k in topk_values:
                accs = []
                for r in domain_stats_runs:
                    c = r[domain]['topk_stats'][k]['correct']
                    t = r[domain]['topk_stats'][k]['total']
                    accs.append((c / t) if t > 0 else 0.0)
                macc_d = statistics.mean(accs)
                sacc_d = statistics.stdev(accs) if len(accs) > 1 else 0.0
                log_stats(f"    TopK-{k}: {macc_d*100:.2f}%Â±{sacc_d*100:.2f}%")
        
        markdown_buffer.write(stats_buffer.getvalue())

    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write(markdown_buffer.getvalue())
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜ä¸ºMarkdown: {summary_file}")

if __name__ == "__main__":
    main()
